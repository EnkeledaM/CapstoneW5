{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Capstone Project - The Battle of Neighborhoods 2", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "\n## Applied Data Science Capstone - IBM/Coursera", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Table of Contents\n\n1. <a href=\"#item1\">Introduction: Business Problem</a>\n2. <a href=\"#item2\">Data</a>  \n3. <a href=\"#item3\">Methodology</a> \n4. <a href=\"#item3\">Analysis</a>\n5. <a href=\"#item4\">Results and Discussion</a>  \n6. <a href=\"#item5\">Conclusion</a>  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Introduction: Business Problem", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "A real estate development and investment company is trying to identify and shortlist retail opportunities in the Greater Toronto area based on trends and popularity.\n\nThe company realizes the importance and relevance of social media in understanding the pulse of the market and seeks to use data as a key driver in decision making.\n\nHow can the company use social trends to select popular venues, understand and identify characteristics of the venues, and select new locations with similar characteristics which would have high growth potential?\nIn this study, as a Data Scientist, I provide a point of view of how data can be acquired, cleansed, curated and analyzed through machine learning technique to better drive the decision-making process.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " # Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nTo drive the understanding and analysis in this data science project, I have used the following data sets:\n\n1-Toronto neighborhoods data from https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M , which was also used in the week 3 assignment. This data set includes the Postal Codes, Boroughs and Neighborhood in the Toronto area starting with the letter M.\n\n2-The above data set was augmented with geo codes for each postal code from the data set provided by Cognitive Class at http://cocl.us/Geospatial_data. Upon merging the data sets, the resulting data set included geo coordinates, i.e. latitude and longitude, for each postal code.\n\n3-Foursquare Places API for Venues \u2013 Foursquare provides various Regular and Premium API endpoints. Regular endpoints include basic venue firmographic data, category, and ID. Premium endpoints include rich content such as ratings, URLs, photos, tips, menus, etc. For the analysis, I have used the \u201cexplore\u201d Regular API endpoint to get venue recommendations via https://developer.foursquare.com/docs/venues/explore.\n\nThe data sets used were already curated and did not require any additionally preparation such as reformatting. The only preparation steps were merging and reshaping of the data frames during the analysis.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Methodology", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Neighborhood Candidate Selection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Fetching package metadata .............\nSolving package specifications: "
                }
            ], 
            "source": "# Import required libraries\n!conda install -c conda-forge folium=0.5.0 --yes\nimport folium\nprint('Folium installed and imported!')\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np  # useful for many scientific computing in Python\nimport pandas as pd # primary data structure library\nimport requests\nfrom geopy.geocoders import Nominatim\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nimport folium\nfrom sklearn.cluster import KMeans\n\n"
        }, 
        {
            "source": "Fetch the neighborhood data from Wikipedia and read it into a DataFrame. Next filter and transform records per specifications.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Read HTML content\ndf = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M')[0][1:]\n\n# Rename columns\ndf.rename(columns={0:'PostalCode',1:'Borough',2:'Neighborhood'},inplace=True)\n\n# Filter dataframe: drop rows with Borough as 'Not assigned'\ndf.drop(df[df.Borough == 'Not assigned'].index, inplace=True)\n\n# Combine neigborhoods that have the same PostalCode and Borough\ngdf = df.groupby(['PostalCode','Borough']).agg(lambda col: ', '.join(col)).reset_index()\n\n# Assign Borough value to Neighborhood that are 'Not assigned'\ngdf.Neighborhood = gdf.Borough.where(gdf.Neighborhood == 'Not assigned',gdf.Neighborhood)\n\nprint('Total number of Neighborhoods: {}'.format(gdf.shape[0]))\ngdf.head()"
        }, 
        {
            "source": "Fetch geocode file and read it into a DataFrame.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "geocodes = pd.read_csv('http://cocl.us/Geospatial_data')\ngeocodes.rename(columns={'Postal Code': 'PostalCode'},inplace=True)\nprint('Total Geo Code entries: {}'.format(geocodes.shape[0]))\ngeocodes.head()"
        }, 
        {
            "source": "Merge the neighborhood and geocode DataFrames.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "neighborhoods = gdf.merge(geocodes, how='left', on=['PostalCode'])\nprint('The dataframe has {} Boroughs and {} Neighborhoods.'.format(\n        len(neighborhoods['Borough'].unique()),\n        neighborhoods.shape[0]\n    )\n)\nneighborhoods.head()"
        }, 
        {
            "source": "Visualize the neighborhoods as markers overlaid in a map of Toronto created using Folium.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create map of city using latitude and longitude values\nmap_city = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, borough, neighborhood in zip(neighborhoods['Latitude'], \n                                           neighborhoods['Longitude'], \n                                           neighborhoods['Borough'], \n                                           neighborhoods['Neighborhood']):\n    label = '{}, {}'.format(neighborhood, borough)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_city)  \nmap_city"
        }, 
        {
            "source": "Fetch venue data from Foursquare for each neighborhood using the \u201cexplore\u201d API endpoint. Aggregate the data in", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Set Foursquare credentials and version\nCLIENT_ID = 'OIJJKMXL2BGR44AP1EFFIGVDGUAL1FDUCNTYHNSI0CSS22NC' # your Foursquare ID\nCLIENT_SECRET = 'OIJJKMXL2BGR44AP1EFFIGVDGUAL1FDUCNTYHNSI0CSS22NC' # your Foursquare Secret\nVERSION = '20180605' # Foursquare API version\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create function to pull venues for a neighborhood using the \"explore\" API endpoint\ndef getNearbyVenues(names, latitudes, longitudes, radius=500, LIMIT=50):\n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['id'], \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Neighborhood', \n                  'Neighborhood Latitude', \n                  'Neighborhood Longitude', \n                  'Venue ID',               \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_venues)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Execute the function to get the list of venues for each neighborhood.\n# Aggregate the data into the city_venues DataFrame\ncity_venues = getNearbyVenues(names=neighborhoods['Neighborhood'],\n                                 latitudes=neighborhoods['Latitude'],\n                                 longitudes=neighborhoods['Longitude']\n                                )\ncity_venues.sort_values(by=['Neighborhood'],inplace=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# OPTIONAL\n# Export to CSV file\ncity_venues.to_csv('city_top_venues.csv')\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# OPTIONAL\n# Read from CSV file\ncity_venues = pd.read_csv('city_top_venues.csv',index_col=0)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print venue and neighborhood record counts\nprint('Pulled {} venues in {} neighborhoods.'.format(\n    city_venues.shape[0],\n    len(city_venues['Neighborhood'].unique())\n))\n\ncity_venues.head()"
        }, 
        {
            "source": "# Exploratory Data Analysis", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Generate statistics from the venue data such as: a) Venue counts by neighborhood b) Top 20 neighborhoods by venue count\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Group venues by neighborhood, aggregate and sort by venue count\ngrouped = city_venues.groupby('Neighborhood').size().reset_index(name='Venue Count by Neighborhood').sort_values(by='Venue Count by Neighborhood',ascending=False)\nprint('Neighborhood Count: {}'.format(neighborhoods.shape[0]))\nprint('Venue Count: {}'.format(city_venues.shape[0]))\nprint('Neighborhoods with venues: {}'.format(len(city_venues['Neighborhood'].unique())))\ngrouped.describe()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "top_neighborhoods = pd.DataFrame(grouped['Neighborhood'][0:20])\ntop_neighborhoods.set_index('Neighborhood',inplace=True)\ntop_neighborhoods.head(20)"
        }, 
        {
            "source": "Visualize the venue data by plotting the number of neighborhoods for each venue count and range of venue counts\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Plot number of Neighborhoods for each Venue Count\nvc='Venue Count by Neighborhood'\nnc='# of Neighborhoods'\ncounted = grouped.groupby(vc).size().reset_index(name=nc).sort_values(by=vc,ascending=True)\nax = counted.plot(kind='bar', x=vc, y=nc, legend=False)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.title('# of Neighborhoods for each Venue Count')\nplt.xlabel('Venue Count')\nplt.ylabel(nc)\nplt.yticks([])\nfor p in ax.patches:\n    ax.annotate(\"%i\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()\n\n# Plot number of Neighborhoods for Venue Count range\nbins=np.arange(0,60,10)\nvc_ranged = counted.rename(columns={vc:'VCR'}).groupby(pd.cut(counted[vc],bins)).sum().drop(columns='VCR').reset_index()\nax = vc_ranged.plot(kind='bar', x=vc, y=nc, legend=False)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.title('# of Neighborhoods for Venue Count Range')\nplt.xlabel('Venue Count Range')\nplt.ylabel(nc)\nplt.yticks([])\nfor p in ax.patches:\n    ax.annotate(\"%i\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()"
        }, 
        {
            "source": "Visualize the top 10 categories across the entire set of venues (all neighborhoods) and as well as the top 20 neighborhoods.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nprint('There are {} uniques categories.'.format(len(city_venues['Venue Category'].unique())))\n\ntop_categories_all = city_venues.groupby('Venue Category').size().reset_index(name='count').sort_values(by='count', ascending=False).reset_index(drop=True)[0:10]\nvc='Venue Category'\nnv='count'\nax = top_categories_all.plot(kind='bar', x=vc, y=nv, legend=False)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.title('# of Venues by Category across all Neighborhoods')\nplt.xlabel('Category')\nplt.ylabel('# of Venues')\nplt.yticks([])\nfor p in ax.patches:\n    ax.annotate(\"%i\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()\n\ntop_categories_top_neighborhoods = city_venues.join(top_neighborhoods,on='Neighborhood',how='inner').groupby('Venue Category').size().reset_index(name='count').sort_values(by='count', ascending=False).reset_index(drop=True)[0:10]\nax = top_categories_top_neighborhoods.plot(kind='bar', x=vc, y=nv, legend=False)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.title('# of Venues by Category across top 20 Neighborhoods')\nplt.xlabel('Category')\nplt.ylabel('# of Venues')\nplt.yticks([])\nfor p in ax.patches:\n    ax.annotate(\"%i\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()"
        }, 
        {
            "source": "One hot encode the data.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# one hot encoding\ncity_onehot = pd.get_dummies(city_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\ncity_onehot['Neighborhood'] = city_venues['Neighborhood'] \n\n# move neighborhood column to the first column\nfixed_columns = [city_onehot.columns[-1]] + list(city_onehot.columns[:-1])\ncity_onehot = city_onehot[fixed_columns]\n\nprint('Dimensions of one hot encoded dataframe: {}'.format(city_onehot.shape))\n\ncity_grouped = city_onehot.groupby('Neighborhood').mean().reset_index()\n\nprint('Dimensions of one hot encoded dataframe grouped by Neighborhood: {}'.format(city_grouped.shape))\nDimensions of one hot encoded dataframe: (1692, 255)\nDimensions of one hot encoded dataframe grouped by Neighborhood: (99, 255)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print each neighborhood along with its respective top 5 most common venues (category) by frequency.\nnum_top_venues = 5\n\nfor hood in city_grouped['Neighborhood']:\n    print(\"----\"+hood+\"----\")\n    temp = city_grouped[city_grouped['Neighborhood'] == hood].T.reset_index()\n    temp.columns = ['venue','freq']\n    temp = temp.iloc[1:]\n    temp['freq'] = temp['freq'].astype(float)\n    temp = temp.round({'freq': 2})\n    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n    print('\\n')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Create a DataFrame containing the top 10 venues by neighborhood based on the one hot encoded data and visualize the data.\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a function to sort the venues in descending order.\ndef return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Call the function to populate the DataFrame with top 10 venues for each neighborhood\nnum_top_venues = 10\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns according to number of top venues\ncolumns = ['Neighborhood']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Venue'.format(ind+1))\n\n# create a new dataframe\nneighborhoods_venues_sorted = pd.DataFrame(columns=columns)\nneighborhoods_venues_sorted['Neighborhood'] = city_grouped['Neighborhood']\n\nfor ind in np.arange(city_grouped.shape[0]):\n    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(city_grouped.iloc[ind, :], num_top_venues)\n\nneighborhoods_venues_sorted.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(\"1st Most Common Venue: {}\".format(neighborhoods_venues_sorted['1st Most Common Venue'].describe().top))\nprint(\"2nd Most Common Venue: {}\".format(neighborhoods_venues_sorted['2nd Most Common Venue'].describe().top))\nprint(\"3rd Most Common Venue: {}\".format(neighborhoods_venues_sorted['3rd Most Common Venue'].describe().top))\nprint(\"4th Most Common Venue: {}\".format(neighborhoods_venues_sorted['4th Most Common Venue'].describe().top))\nprint(\"5th Most Common Venue: {}\".format(neighborhoods_venues_sorted['5th Most Common Venue'].describe().top))\nprint(\"6th Most Common Venue: {}\".format(neighborhoods_venues_sorted['6th Most Common Venue'].describe().top))\nprint(\"7th Most Common Venue: {}\".format(neighborhoods_venues_sorted['7th Most Common Venue'].describe().top))\nprint(\"8th Most Common Venue: {}\".format(neighborhoods_venues_sorted['8th Most Common Venue'].describe().top))\nprint(\"9th Most Common Venue: {}\".format(neighborhoods_venues_sorted['9th Most Common Venue'].describe().top))\nprint(\"10th Most Common Venue: {}\".format(neighborhoods_venues_sorted['10th Most Common Venue'].describe().top))"
        }, 
        {
            "source": "# Analysis (Machine Learning)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Based on the data available for neighborhoods and venues, we can define venue categories as features for machine learning. Given there are approximately 166 categories across a data set of 99 neighborhoods, use of k-means clustering to cluster the neighborhoods sounds like a reasonable approach. The 166 categories naturally map to features used in the k-means model. An initial value of \u2018k\u2019 was set to 7 [square root of 49.5 (99 divided by 2)]. This generates the cluster labels for each of the neighborhoods.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# set number of clusters\nkclusters = 7\n\ncity_grouped_clustering = city_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(city_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# add clustering labels\nneighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n\ncity_merged = neighborhoods\n\n# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\ncity_merged = city_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n\n# drop rows with no assigned clusters\ncity_merged.dropna(inplace=True)\n\ncity_merged.head() # check the last columns!"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Visualize the resulting clusters on a map of Toronto using Folium.\nIn [24]:\n# create map\nmap_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(city_merged['Latitude'], city_merged['Longitude'], city_merged['Neighborhood'], city_merged['Cluster Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    cluster = int(cluster)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=rainbow[cluster-1],\n        fill=True,\n        fill_color=rainbow[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\n       \nmap_clusters\n\n"
        }, 
        {
            "source": "# Results\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this section we examine the clusters and view cluster specific details.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Plot the distribution of neighborhoods across clusters\ncn = city_merged.groupby('Cluster Labels').size().reset_index(name='# of Neighborhoods')\nax = cn.plot(kind='bar', x='Cluster Labels', y='# of Neighborhoods', legend=False)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.title('Distribution of Neighborhoods by cluster')\nplt.xlabel('Cluster')\nplt.ylabel('# of Neighborhoods')\nplt.yticks([])\nfor p in ax.patches:\n    ax.annotate(\"%i\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()\n\n# Plot the distribution of venues across clusters\ncv = city_venues.merge(city_merged, on='Neighborhood')[['Venue','Cluster Labels']].groupby('Cluster Labels').size().reset_index(name='# of Venues')\nax = cv.plot(kind='bar', x='Cluster Labels', y='# of Venues', legend=False)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.title('Distribution of Venues by cluster')\nplt.xlabel('Cluster')\nplt.ylabel('# of Venues')\nplt.yticks([])\nfor p in ax.patches:\n    ax.annotate(\"%i\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')\nplt.show()\n\n# Display most common venue category in each cluster\ncity_venues.merge(city_merged, on='Neighborhood')[['Cluster Labels','Venue Category']].groupby(['Cluster Labels','Venue Category']).size().reset_index(name='Count').sort_values(by=['Cluster Labels','Count'],ascending=[True,False]).groupby(['Cluster Labels']).head(1)"
        }, 
        {
            "source": "The plots above present the distribution of entire set of neighborhoods (99) and venues (1692) across 7 clusters. Each cluster includes neighborhoods that have commonality based on the feature set which equates to the categories of the venues in the neighborhood. A key point to notice is the uneven distribution of the neighborhoods and venues across the clusters indicating the similarity or cohesiveness in the clusters 5 and 6.\n\nNext, we look at the most common venue in each cluster. Coffee Shops and Fast Food Restaurants are the most common venues in clusters 5 and 6 respectively.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "city_merged.loc[city_merged['Cluster Labels'] == 0, city_merged.columns[[1] + [2] + list(range(5, city_merged.shape[1]))]]"
        }, 
        {
            "source": "Cluster 6 is the second largest cluster with 34 neighborhoods and includes 439 venues. Like cluster 5, it covers most of the boroughs. The most common venues is Fast Food Restaurant. Apart from restaurants, this cluster includes a wide range of retails outlets.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Discussion", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "I observed the following during the analysis of the results:\n\nPredominance of 2 clusters across the neighborhoods and venue categories which indicates similarity or commonality of features. The remaining 4 clusters had more distinguishing or unique features.\n\nThe k-means clustering approach relied of frequency of a category across the 255 unique categories. The feature set may be large compared to the number of samples, i.e. number of neighborhoods (99).\n\nI tried multiple values of k in the k-means clustering. For lower values of k, the larger clusters coalesced into a single cluster. For higher values of k, the number of smaller clusters increased but the larger clusters did not break up noticeably any further.\n\nThe Foursquare data is primarily social and is crowdsourced. I noticed the API calls returned slightly different data sets when executed at various times of the day or day of the week.\n\nBased on the results, I have the following recommendations:\n\nI had planned initially to use the Premium Endpoint to fetch ratings but was unable to because of the daily limits of API calls. This extended data could have provided a social dimension, but the data would change frequently.\n\nRunning the analysis and comparing results over a period as opposed to a snapshot would stabilize the findings.\nConsider other unsupervised learning methods for comparative analysis.\nAugment demographic data for neighborhoods to get additional insights.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Conclusion\n\nIn conclusion, this study was a positive step for the stakeholders to understand how data from various sources can be used via powerful tools and visualization techniques to derive insights.\n\nFrom a personal perspective, it provided me with exposure to the data science methodology from a business problem, analysis, data acquisition, preparation, feature selection, model creation, train/fit and test/analyze results. The libraries for data acquisition, preparation, and visualization demonstrated the value of data science.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}